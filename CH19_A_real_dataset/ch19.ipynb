{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Real Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label | Description |\n",
    "|-------|-------------|\n",
    "| 0     | T-shirt/top |\n",
    "| 1     | Trouser     |\n",
    "| 2     | Pullover    |\n",
    "| 3     | Dress       |\n",
    "| 4     | Coat        |\n",
    "| 5     | Sandal      |\n",
    "| 6     | Shirt       |\n",
    "| 7     | Sneaker     |\n",
    "| 8     | Bag         |\n",
    "| 9     | Ankle boot  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and CONSTANTS\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "URL = \"https://nnfs.io/datasets/fashion_mnist_images.zip\"\n",
    "FILE = \"fashion_mnist_images.zip\"\n",
    "FOLDER = \"fashion_mnist_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://nnfs.io/datasets/fashion_mnist_images.zip and saving as fashion_mnist_images.zip...\n",
      "Unzipping images...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "if not os.path.isfile(FILE):\n",
    "    print(f\"Downloading {URL} and saving as {FILE}...\")\n",
    "    urllib.request.urlretrieve(URL, FILE)\n",
    "\n",
    "print(\"Unzipping images...\")\n",
    "with ZipFile(FILE) as zip_images:\n",
    "    zip_images.extractall(FOLDER)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9', '0', '7', '6', '1', '8', '4', '3', '2', '5']\n",
      "['3975.png', '1804.png', '4968.png', '1810.png', '3961.png', '0298.png', '1186.png', '3949.png', '3791.png', '4798.png']\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "labels = os.listdir(\"fashion_mnist_images/train\")\n",
    "print(labels)\n",
    "\n",
    "files = os.listdir(\"fashion_mnist_images/train/0\")\n",
    "print(files[:10])\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  49 135 182 150  59   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  78 255 220 212 219 255 246 191 155  87   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  57 206 215 203 191 203 212 216 217 220 211  15   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0  58 231 220 210 199 209 218 218 217 208 200 215  56   0]\n",
      " [  0   0   0   0   1   2   0   0   4   0   0   0   0 145 213 207 199 187 203 210 216 217 215 215 206 215 130   0]\n",
      " [  0   0   0   0   1   2   4   0   0   0   3 105 225 205 190 201 210 214 213 215 215 212 211 208 205 207 218   0]\n",
      " [  1   5   7   0   0   0   0   0  52 162 217 189 174 157 187 198 202 217 220 223 224 222 217 211 217 201 247  65]\n",
      " [  0   0   0   0   0   0  21  72 185 189 171 171 185 203 200 207 208 209 214 219 222 222 224 215 218 211 212 148]\n",
      " [  0  70 114 129 145 159 179 196 172 176 185 196 199 206 201 210 212 213 216 218 219 217 212 207 208 200 198 173]\n",
      " [  0 122 158 184 194 192 193 196 203 209 211 211 215 218 221 222 226 227 227 226 226 223 222 216 211 208 216 185]\n",
      " [ 21   0   0  12  48  82 123 152 170 184 195 211 225 232 233 237 242 242 240 240 238 236 222 209 200 193 185 106]\n",
      " [ 26  47  54  18   5   0   0   0   0   0   0   0   0   0   2   4   6   9   9   8   9   6   6   4   2   0   0   0]\n",
      " [  0  10  27  45  55  59  57  50  44  51  58  62  65  56  54  57  59  61  60  63  68  67  66  73  77  74  65  39]\n",
      " [  0   0   0   0   4   9  18  23  26  25  23  25  29  37  38  37  39  36  29  31  33  34  28  24  20  14   7   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=200)\n",
    "\n",
    "image_data = cv2.imread(\"fashion_mnist_images/train/7/0002.png\", cv2.IMREAD_UNCHANGED)\n",
    "print(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_data = cv2.imread(\"fashion_mnist_images/train/4/0011.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "plt.imshow(image_data, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset, path):\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "\n",
    "    # Create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            # Read the image\n",
    "            image = cv2.imread(\n",
    "                os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED\n",
    "            )\n",
    "\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "\n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype(\"uint8\")\n",
    "\n",
    "\n",
    "# MNIST dataset (train + test)\n",
    "def create_data_mnist(path):\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset(\"train\", path)\n",
    "    X_test, y_test = load_mnist_dataset(\"test\", path)\n",
    "\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_inputs,\n",
    "        n_neurons,\n",
    "        weight_regularizer_l1=0,\n",
    "        weight_regularizer_l2=0,\n",
    "        bias_regularizer_l1=0,\n",
    "        bias_regularizer_l2=0,\n",
    "    ):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # If not in the training mode - return values\n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = (\n",
    "            np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        )\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "# Input \"layer\"\n",
    "class Layer_Input:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(\n",
    "            zip(self.output, dvalues)\n",
    "        ):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(\n",
    "                single_output, single_output.T\n",
    "            )\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "\n",
    "\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "\n",
    "\n",
    "# Linear activation\n",
    "class Activation_Linear:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = (\n",
    "                self.momentum * layer.weight_momentums\n",
    "                - self.current_learning_rate * layer.dweights\n",
    "            )\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = (\n",
    "                self.momentum * layer.bias_momentums\n",
    "                - self.current_learning_rate * layer.dbiases\n",
    "            )\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dweights\n",
    "            / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        )\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dbiases\n",
    "            / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0.0, epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = (\n",
    "            self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        )\n",
    "        layer.bias_cache = (\n",
    "            self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "        )\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dweights\n",
    "            / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        )\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dbiases\n",
    "            / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(\n",
    "        self, learning_rate=0.001, decay=0.0, epsilon=1e-7, beta_1=0.9, beta_2=0.999\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = (\n",
    "            self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        )\n",
    "        layer.bias_momentums = (\n",
    "            self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        )\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (\n",
    "            1 - self.beta_1 ** (self.iterations + 1)\n",
    "        )\n",
    "        bias_momentums_corrected = layer.bias_momentums / (\n",
    "            1 - self.beta_1 ** (self.iterations + 1)\n",
    "        )\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = (\n",
    "            self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        )\n",
    "        layer.bias_cache = (\n",
    "            self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        )\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (\n",
    "            1 - self.beta_2 ** (self.iterations + 1)\n",
    "        )\n",
    "        bias_cache_corrected = layer.bias_cache / (\n",
    "            1 - self.beta_2 ** (self.iterations + 1)\n",
    "        )\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * weight_momentums_corrected\n",
    "            / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        )\n",
    "\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * bias_momentums_corrected\n",
    "            / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self):\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # Calculate regularization loss\n",
    "        # iterate all trainable layers\n",
    "        for layer in self.trainable_layers:\n",
    "            # L1 regularization - weights\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(\n",
    "                    np.abs(layer.weights)\n",
    "                )\n",
    "\n",
    "            # L2 regularization - weights\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(\n",
    "                    layer.weights * layer.weights\n",
    "                )\n",
    "\n",
    "            # L1 regularization - biases\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(\n",
    "                    np.abs(layer.biases)\n",
    "                )\n",
    "\n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(\n",
    "                    layer.biases * layer.biases\n",
    "                )\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    # Set/remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Add accumulated sum of losses and sample count\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Calculates accumulated loss\n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "        # Calculate mean loss\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Reset variables for accumulated loss\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(\n",
    "            y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped)\n",
    "        )\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = (\n",
    "            -(y_true / clipped_dvalues - (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        )\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss):  # L2 loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred) ** 2, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss):  # L1 loss\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Common accuracy class\n",
    "class Accuracy:\n",
    "    # Calculates an accuracy\n",
    "    # given predictions and ground truth values\n",
    "    def calculate(self, predictions, y):\n",
    "        # Get comparison results\n",
    "        comparisons = self.compare(predictions, y)\n",
    "\n",
    "        # Calculate an accuracy\n",
    "        accuracy = np.mean(comparisons)\n",
    "\n",
    "        # Add accumulated sum of matching values and sample count\n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "\n",
    "        # Return accuracy\n",
    "        return accuracy\n",
    "\n",
    "    # Calculates accumulated accuracy\n",
    "    def calculate_accumulated(self):\n",
    "        # Calculate an accuracy\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return accuracy\n",
    "\n",
    "    # Reset variables for accumulated accuracy\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "\n",
    "# Accuracy calculation for classification model\n",
    "class Accuracy_Categorical(Accuracy):\n",
    "    def __init__(self, *, binary=False):\n",
    "        # Binary mode?\n",
    "        self.binary = binary\n",
    "\n",
    "    # No initialization is needed\n",
    "    def init(self, y):\n",
    "        pass\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        if not self.binary and len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        return predictions == y\n",
    "\n",
    "\n",
    "# Accuracy calculation for regression model\n",
    "class Accuracy_Regression(Accuracy):\n",
    "    def __init__(self):\n",
    "        # Create precision property\n",
    "        self.precision = None\n",
    "\n",
    "    # Calculates precision value\n",
    "    # based on passed-in ground truth values\n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    # Add objects to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "    # Finalize the model\n",
    "    def finalize(self):\n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "\n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "\n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "\n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], \"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "        # Update loss object with trainable layers\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "\n",
    "        # If output activation is Softmax and\n",
    "        # loss function is Categorical Cross-Entropy\n",
    "        # create an object of combined activation\n",
    "        # and loss function containing\n",
    "        # faster gradient calculation\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(\n",
    "            self.loss, Loss_CategoricalCrossentropy\n",
    "        ):\n",
    "            # Create an object of combined activation\n",
    "            # and loss functions\n",
    "            self.softmax_classifier_output = (\n",
    "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "            )\n",
    "\n",
    "    # Train the model\n",
    "    def train(\n",
    "        self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data=None\n",
    "    ):\n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        # Default value if batch size is not being set\n",
    "        train_steps = 1\n",
    "\n",
    "        # If there is validation data passed,\n",
    "        # set default number of steps for validation as well\n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "\n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            # Dividing rounds down. If there are some remaining\n",
    "            # data but not a full batch, this won't include it\n",
    "            # Add `1` to include this not full batch\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "\n",
    "                # Dividing rounds down. If there are some remaining\n",
    "                # data but nor full batch, this won't include it\n",
    "                # Add `1` to include this not full batch\n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Print epoch number\n",
    "            print(f\"epoch: {epoch}\")\n",
    "\n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            # Iterate over steps\n",
    "            for step in range(train_steps):\n",
    "                # If batch size is not set -\n",
    "                # train using one step and full dataset\n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "\n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size : (step + 1) * batch_size]\n",
    "                    batch_y = y[step * batch_size : (step + 1) * batch_size]\n",
    "\n",
    "                # Perform the forward pass\n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                # Calculate loss\n",
    "                data_loss, regularization_loss = self.loss.calculate(\n",
    "                    output, batch_y, include_regularization=True\n",
    "                )\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                # Get predictions and calculate an accuracy\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                # Perform backward pass\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "                # Optimize (update parameters)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                # Print a summary\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(\n",
    "                        f\"step: {step}, \"\n",
    "                        + f\"acc: {accuracy:.3f}, \"\n",
    "                        + f\"loss: {loss:.3f} (\"\n",
    "                        + f\"data_loss: {data_loss:.3f}, \"\n",
    "                        + f\"reg_loss: {regularization_loss:.3f}), \"\n",
    "                        + f\"lr: {self.optimizer.current_learning_rate}\"\n",
    "                    )\n",
    "\n",
    "            # Get and print epoch loss and accuracy\n",
    "            (\n",
    "                epoch_data_loss,\n",
    "                epoch_regularization_loss,\n",
    "            ) = self.loss.calculate_accumulated(include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "            print(\n",
    "                f\"training, \"\n",
    "                + f\"acc: {epoch_accuracy:.3f}, \"\n",
    "                + f\"loss: {epoch_loss:.3f} (\"\n",
    "                + f\"data_loss: {epoch_data_loss:.3f}, \"\n",
    "                + f\"reg_loss: {epoch_regularization_loss:.3f}), \"\n",
    "                + f\"lr: {self.optimizer.current_learning_rate}\"\n",
    "            )\n",
    "\n",
    "            # If there is the validation data\n",
    "            if validation_data is not None:\n",
    "                # Reset accumulated values in loss\n",
    "                # and accuracy objects\n",
    "                self.loss.new_pass()\n",
    "                self.accuracy.new_pass()\n",
    "\n",
    "                # Iterate over steps\n",
    "                for step in range(validation_steps):\n",
    "                    # If batch size is not set -\n",
    "                    # train using one step and full dataset\n",
    "                    if batch_size is None:\n",
    "                        batch_X = X_val\n",
    "                        batch_y = y_val\n",
    "\n",
    "                    # Otherwise slice a batch\n",
    "                    else:\n",
    "                        batch_X = X_val[step * batch_size : (step + 1) * batch_size]\n",
    "                        batch_y = y_val[step * batch_size : (step + 1) * batch_size]\n",
    "\n",
    "                    # Perform the forward pass\n",
    "                    output = self.forward(batch_X, training=False)\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    self.loss.calculate(output, batch_y)\n",
    "\n",
    "                    # Get predictions and calculate an accuracy\n",
    "                    predictions = self.output_layer_activation.predictions(output)\n",
    "                    self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                # Get and print validation loss and accuracy\n",
    "                validation_loss = self.loss.calculate_accumulated()\n",
    "                validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "                # Print a summary\n",
    "                print(\n",
    "                    f\"validation, \"\n",
    "                    + f\"acc: {validation_accuracy:.3f}, \"\n",
    "                    + f\"loss: {validation_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward(self, X, training):\n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "\n",
    "    # Performs backward pass\n",
    "    def backward(self, output, y):\n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
    "\n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "\n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "\n",
    "\n",
    "# Loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset, path):\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "\n",
    "    # Create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            # Read the image\n",
    "            image = cv2.imread(\n",
    "                os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED\n",
    "            )\n",
    "\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "\n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype(\"uint8\")\n",
    "\n",
    "\n",
    "# MNIST dataset (train + test)\n",
    "def create_data_mnist(path):\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset(\"train\", path)\n",
    "    X_test, y_test = load_mnist_dataset(\"test\", path)\n",
    "\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist(\"fashion_mnist_images\")\n",
    "\n",
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Model()\n",
    "\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    optimizer=Optimizer_Adam(decay=1e-3),\n",
    "    accuracy=Accuracy_Categorical(),\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.102, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.750, loss: 0.649 (data_loss: 0.649, reg_loss: 0.000), lr: 0.0009090909090909091\n",
      "step: 200, acc: 0.789, loss: 0.510 (data_loss: 0.510, reg_loss: 0.000), lr: 0.0008333333333333334\n",
      "step: 300, acc: 0.852, loss: 0.524 (data_loss: 0.524, reg_loss: 0.000), lr: 0.0007692307692307692\n",
      "step: 400, acc: 0.852, loss: 0.425 (data_loss: 0.425, reg_loss: 0.000), lr: 0.0007142857142857143\n",
      "step: 468, acc: 0.812, loss: 0.547 (data_loss: 0.547, reg_loss: 0.000), lr: 0.000681198910081744\n",
      "training, acc: 0.762, loss: 0.651 (data_loss: 0.651, reg_loss: 0.000), lr: 0.000681198910081744\n",
      "validation, acc: 0.825, loss: 0.482\n",
      "epoch: 2\n",
      "step: 0, acc: 0.852, loss: 0.427 (data_loss: 0.427, reg_loss: 0.000), lr: 0.0006807351940095304\n",
      "step: 100, acc: 0.836, loss: 0.435 (data_loss: 0.435, reg_loss: 0.000), lr: 0.0006373486297004461\n",
      "step: 200, acc: 0.852, loss: 0.368 (data_loss: 0.368, reg_loss: 0.000), lr: 0.0005991611743559018\n",
      "step: 300, acc: 0.883, loss: 0.430 (data_loss: 0.430, reg_loss: 0.000), lr: 0.0005652911249293386\n",
      "step: 400, acc: 0.852, loss: 0.371 (data_loss: 0.371, reg_loss: 0.000), lr: 0.0005350454788657037\n",
      "step: 468, acc: 0.781, loss: 0.489 (data_loss: 0.489, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "training, acc: 0.847, loss: 0.424 (data_loss: 0.424, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "validation, acc: 0.847, loss: 0.426\n",
      "epoch: 3\n",
      "step: 0, acc: 0.859, loss: 0.370 (data_loss: 0.370, reg_loss: 0.000), lr: 0.0005159958720330237\n",
      "step: 100, acc: 0.852, loss: 0.389 (data_loss: 0.389, reg_loss: 0.000), lr: 0.0004906771344455348\n",
      "step: 200, acc: 0.867, loss: 0.343 (data_loss: 0.343, reg_loss: 0.000), lr: 0.0004677268475210477\n",
      "step: 300, acc: 0.891, loss: 0.403 (data_loss: 0.403, reg_loss: 0.000), lr: 0.00044682752457551384\n",
      "step: 400, acc: 0.859, loss: 0.347 (data_loss: 0.347, reg_loss: 0.000), lr: 0.00042771599657827206\n",
      "step: 468, acc: 0.792, loss: 0.459 (data_loss: 0.459, reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "training, acc: 0.861, loss: 0.382 (data_loss: 0.382, reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "validation, acc: 0.856, loss: 0.399\n",
      "epoch: 4\n",
      "step: 0, acc: 0.875, loss: 0.336 (data_loss: 0.336, reg_loss: 0.000), lr: 0.0004154549231408392\n",
      "step: 100, acc: 0.859, loss: 0.379 (data_loss: 0.379, reg_loss: 0.000), lr: 0.00039888312724371757\n",
      "step: 200, acc: 0.867, loss: 0.323 (data_loss: 0.323, reg_loss: 0.000), lr: 0.0003835826620636747\n",
      "step: 300, acc: 0.891, loss: 0.373 (data_loss: 0.373, reg_loss: 0.000), lr: 0.0003694126339120798\n",
      "step: 400, acc: 0.867, loss: 0.339 (data_loss: 0.339, reg_loss: 0.000), lr: 0.0003562522265764161\n",
      "step: 468, acc: 0.792, loss: 0.432 (data_loss: 0.432, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "training, acc: 0.870, loss: 0.357 (data_loss: 0.357, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "validation, acc: 0.862, loss: 0.386\n",
      "epoch: 5\n",
      "step: 0, acc: 0.891, loss: 0.311 (data_loss: 0.311, reg_loss: 0.000), lr: 0.0003477051460361613\n",
      "step: 100, acc: 0.867, loss: 0.368 (data_loss: 0.368, reg_loss: 0.000), lr: 0.00033602150537634406\n",
      "step: 200, acc: 0.875, loss: 0.310 (data_loss: 0.310, reg_loss: 0.000), lr: 0.00032509752925877764\n",
      "step: 300, acc: 0.898, loss: 0.342 (data_loss: 0.342, reg_loss: 0.000), lr: 0.00031486146095717883\n",
      "step: 400, acc: 0.867, loss: 0.327 (data_loss: 0.327, reg_loss: 0.000), lr: 0.00030525030525030525\n",
      "step: 468, acc: 0.802, loss: 0.406 (data_loss: 0.406, reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "training, acc: 0.876, loss: 0.340 (data_loss: 0.340, reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "validation, acc: 0.865, loss: 0.376\n",
      "epoch: 6\n",
      "step: 0, acc: 0.906, loss: 0.288 (data_loss: 0.288, reg_loss: 0.000), lr: 0.0002989536621823617\n",
      "step: 100, acc: 0.875, loss: 0.360 (data_loss: 0.360, reg_loss: 0.000), lr: 0.00029027576197387516\n",
      "step: 200, acc: 0.875, loss: 0.300 (data_loss: 0.300, reg_loss: 0.000), lr: 0.0002820874471086037\n",
      "step: 300, acc: 0.906, loss: 0.321 (data_loss: 0.321, reg_loss: 0.000), lr: 0.00027434842249657066\n",
      "step: 400, acc: 0.891, loss: 0.311 (data_loss: 0.311, reg_loss: 0.000), lr: 0.000267022696929239\n",
      "step: 468, acc: 0.812, loss: 0.389 (data_loss: 0.389, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "training, acc: 0.881, loss: 0.326 (data_loss: 0.326, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "validation, acc: 0.868, loss: 0.368\n",
      "epoch: 7\n",
      "step: 0, acc: 0.922, loss: 0.267 (data_loss: 0.267, reg_loss: 0.000), lr: 0.00026219192448872575\n",
      "step: 100, acc: 0.867, loss: 0.353 (data_loss: 0.353, reg_loss: 0.000), lr: 0.00025549310168625444\n",
      "step: 200, acc: 0.883, loss: 0.292 (data_loss: 0.292, reg_loss: 0.000), lr: 0.00024912805181863477\n",
      "step: 300, acc: 0.906, loss: 0.305 (data_loss: 0.305, reg_loss: 0.000), lr: 0.0002430724355858046\n",
      "step: 400, acc: 0.891, loss: 0.300 (data_loss: 0.300, reg_loss: 0.000), lr: 0.00023730422401518745\n",
      "step: 468, acc: 0.812, loss: 0.373 (data_loss: 0.373, reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "training, acc: 0.885, loss: 0.315 (data_loss: 0.315, reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "validation, acc: 0.870, loss: 0.362\n",
      "epoch: 8\n",
      "step: 0, acc: 0.914, loss: 0.252 (data_loss: 0.252, reg_loss: 0.000), lr: 0.00023348120476301658\n",
      "step: 100, acc: 0.883, loss: 0.346 (data_loss: 0.346, reg_loss: 0.000), lr: 0.00022815423226100847\n",
      "step: 200, acc: 0.883, loss: 0.286 (data_loss: 0.286, reg_loss: 0.000), lr: 0.0002230649118893598\n",
      "step: 300, acc: 0.906, loss: 0.296 (data_loss: 0.296, reg_loss: 0.000), lr: 0.00021819768710451667\n",
      "step: 400, acc: 0.898, loss: 0.290 (data_loss: 0.290, reg_loss: 0.000), lr: 0.00021353833013025838\n",
      "step: 468, acc: 0.823, loss: 0.360 (data_loss: 0.360, reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "training, acc: 0.888, loss: 0.307 (data_loss: 0.307, reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "validation, acc: 0.872, loss: 0.358\n",
      "epoch: 9\n",
      "step: 0, acc: 0.914, loss: 0.239 (data_loss: 0.239, reg_loss: 0.000), lr: 0.0002104377104377104\n",
      "step: 100, acc: 0.883, loss: 0.340 (data_loss: 0.340, reg_loss: 0.000), lr: 0.0002061005770816158\n",
      "step: 200, acc: 0.891, loss: 0.279 (data_loss: 0.279, reg_loss: 0.000), lr: 0.00020193861066235866\n",
      "step: 300, acc: 0.906, loss: 0.289 (data_loss: 0.289, reg_loss: 0.000), lr: 0.0001979414093428345\n",
      "step: 400, acc: 0.898, loss: 0.281 (data_loss: 0.281, reg_loss: 0.000), lr: 0.0001940993788819876\n",
      "step: 468, acc: 0.833, loss: 0.348 (data_loss: 0.348, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "training, acc: 0.891, loss: 0.299 (data_loss: 0.299, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "validation, acc: 0.874, loss: 0.354\n",
      "epoch: 10\n",
      "step: 0, acc: 0.914, loss: 0.227 (data_loss: 0.227, reg_loss: 0.000), lr: 0.0001915341888527102\n",
      "step: 100, acc: 0.883, loss: 0.334 (data_loss: 0.334, reg_loss: 0.000), lr: 0.00018793459875963167\n",
      "step: 200, acc: 0.891, loss: 0.273 (data_loss: 0.273, reg_loss: 0.000), lr: 0.00018446781036709093\n",
      "step: 300, acc: 0.898, loss: 0.284 (data_loss: 0.284, reg_loss: 0.000), lr: 0.00018112660749864155\n",
      "step: 400, acc: 0.898, loss: 0.272 (data_loss: 0.272, reg_loss: 0.000), lr: 0.00017790428749332856\n",
      "step: 468, acc: 0.854, loss: 0.340 (data_loss: 0.340, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "training, acc: 0.893, loss: 0.293 (data_loss: 0.293, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "validation, acc: 0.875, loss: 0.351\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    X, y, validation_data=(X_test, y_test), epochs=10, batch_size=128, print_every=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
