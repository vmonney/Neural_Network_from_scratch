{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Network Error with Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "# an example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "# Ground truth\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = -(\n",
    "    math.log(softmax_output[0]) * target_output[0]\n",
    "    + math.log(softmax_output[1]) * target_output[1]\n",
    "    + math.log(softmax_output[2]) * target_output[2]\n",
    ")\n",
    "\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# similarly\n",
    "loss = -math.log(softmax_output[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Categorical Cross-Entropy Loss accounts for the fact that the output is a probability distribution and outputs a larger loss the lower the confidence is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-0.05129329438755058\n",
      "-0.10536051565782628\n",
      "-0.2231435513142097\n",
      "...\n",
      "-1.6094379124341003\n",
      "-2.3025850929940455\n",
      "-2.995732273553991\n",
      "-4.605170185988091\n"
     ]
    }
   ],
   "source": [
    "print(math.log(1))\n",
    "print(math.log(0.95))\n",
    "print(math.log(0.9))\n",
    "print(math.log(0.8))\n",
    "print(\"...\")\n",
    "print(math.log(0.2))\n",
    "print(math.log(0.1))\n",
    "print(math.log(0.05))\n",
    "print(math.log(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n",
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "b = 5.2\n",
    "print(np.log(b))\n",
    "print(math.e ** np.log(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "softmax_output = [[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]]\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "for targ_idx, distribution in zip(class_targets, softmax_output):\n",
    "    print(distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "# same using numpy\n",
    "softmax_output = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "print(softmax_output[[0, 1, 2], class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 0.5, 0.9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_output[range(len(softmax_output)), class_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# Now we apply the negative log to this list\n",
    "neg_log = -np.log(softmax_output[range(len(softmax_output)), class_targets])\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3 , 0.9 , 0.98])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(softmax_output * class_targets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# Handle if target are not sparse but one-hot encoded\n",
    "softmax_output = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([[1, 0, 0], [0, 1, 0], [0, 1, 0]])\n",
    "\n",
    "# Probabilities for target values only if categorical labels\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_output[range(len(softmax_output)), class_targets]\n",
    "\n",
    "# Mask values - only for one-hot encoded labels\n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(softmax_output * class_targets, axis=1)\n",
    "\n",
    "# Losses\n",
    "neg_log = -np.log(correct_confidences)\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Categorical Cross-Entropy Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_output, class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining everything up to this point: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "Hey Um sample losees:  [1.0986123 1.0986128 1.0986137 1.0986145 1.0986153 1.0986137 1.0986168\n",
      " 1.0986168 1.0986178 1.0986191 1.0986198 1.098619  1.0986207 1.098622\n",
      " 1.098622  1.0986223 1.0986184 1.0986241 1.0986224 1.098626  1.098626\n",
      " 1.0986195 1.0986171 1.0986255 1.0986353 1.0986279 1.098618  1.0986184\n",
      " 1.0986398 1.0986418 1.0986305 1.0986369 1.0986211 1.0986254 1.0986377\n",
      " 1.098647  1.0986525 1.098653  1.0986506 1.098654  1.0986497 1.0986483\n",
      " 1.0986478 1.0986323 1.0986618 1.098662  1.098664  1.0986414 1.0986663\n",
      " 1.0986534 1.098662  1.0986348 1.0986499 1.0986599 1.0986315 1.0986141\n",
      " 1.0986186 1.0986127 1.0986273 1.0986137 1.098617  1.098616  1.0986123\n",
      " 1.098631  1.0986419 1.0986344 1.0986123 1.0986576 1.0986377 1.0986589\n",
      " 1.0986655 1.0986648 1.0986633 1.0986542 1.0986674 1.0986677 1.0986685\n",
      " 1.0986708 1.0986696 1.0986639 1.0986731 1.0986444 1.0986496 1.0986748\n",
      " 1.0986496 1.0986749 1.0986547 1.0986434 1.0986588 1.0986764 1.0986344\n",
      " 1.0986977 1.0986687 1.0987039 1.098692  1.0987091 1.0986956 1.0987171\n",
      " 1.0987128 1.098722  1.0986123 1.0986127 1.0986134 1.0986152 1.0986154\n",
      " 1.0986152 1.0986184 1.0986192 1.0986148 1.0986161 1.0986137 1.0986179\n",
      " 1.0986241 1.098613  1.0986227 1.0986148 1.0986123 1.0986198 1.0986123\n",
      " 1.0986149 1.0986127 1.0986229 1.0986152 1.0986241 1.0986125 1.098617\n",
      " 1.0986165 1.0986314 1.0986289 1.0986291 1.0986218 1.098633  1.0986272\n",
      " 1.0986339 1.0986319 1.0986371 1.0986376 1.0986384 1.0986383 1.0986383\n",
      " 1.0986379 1.0986363 1.0986376 1.0986311 1.0986297 1.0986218 1.098642\n",
      " 1.0986164 1.0986434 1.0986321 1.0986229 1.0986484 1.098628  1.0986263\n",
      " 1.0986216 1.0986159 1.0986596 1.0986164 1.098617  1.0986413 1.0986454\n",
      " 1.098671  1.0986749 1.0986711 1.0986499 1.098677  1.098666  1.0986592\n",
      " 1.0986667 1.0986786 1.0986652 1.0986745 1.0986577 1.0986841 1.0986863\n",
      " 1.0986457 1.098681  1.0986754 1.0986645 1.0986422 1.0986489 1.0986756\n",
      " 1.0986454 1.0986873 1.0986316 1.0986601 1.0986327 1.0986334 1.0986155\n",
      " 1.0986735 1.0986123 1.0986614 1.0986596 1.0986346 1.0986767 1.0986708\n",
      " 1.0986806 1.0986805 1.0986531 1.0986693 1.0986123 1.0986109 1.09861\n",
      " 1.0986086 1.0986077 1.0986053 1.0986048 1.0986024 1.0986047 1.0986029\n",
      " 1.098609  1.0986058 1.0986084 1.0986017 1.0985988 1.0986042 1.098607\n",
      " 1.0985962 1.0985739 1.0985948 1.0986065 1.0985909 1.0985881 1.0985682\n",
      " 1.0986019 1.0985645 1.0985594 1.0985554 1.0985595 1.0985516 1.0985633\n",
      " 1.0985466 1.098544  1.0985512 1.0985405 1.0985655 1.098593  1.0985372\n",
      " 1.0985689 1.0985836 1.0985469 1.0985584 1.0985733 1.0985739 1.0985808\n",
      " 1.0985379 1.0985814 1.0985769 1.098592  1.0985705 1.0985731 1.0986037\n",
      " 1.0985708 1.0985787 1.0985433 1.0985383 1.0985698 1.0985862 1.0985857\n",
      " 1.0985255 1.0985543 1.0985299 1.0985267 1.0985194 1.0985483 1.098522\n",
      " 1.0985324 1.0985252 1.0985197 1.0985336 1.0985484 1.0985223 1.0985361\n",
      " 1.0985708 1.098536  1.0985528 1.0985816 1.0985836 1.0985138 1.0984505\n",
      " 1.0985631 1.0984706 1.0985769 1.0984368 1.0984722 1.098446  1.0985355\n",
      " 1.098427  1.0984246 1.0984327 1.098446  1.0984297 1.0985247 1.0984324\n",
      " 1.0984129 1.0984786 1.0984267 1.0984656 1.0984188 1.098476 ]\n",
      "loss: 1.0986104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n>>>\\n[[0.33333334 0.33333334 0.33333334]\\n [0.33333316 0.3333332  0.33333364]\\n [0.33333287 0.3333329  0.33333418]\\n [0.3333326  0.33333263 0.33333477]\\n [0.33333233 0.3333324  0.33333528]]\\nloss: 1.0986104\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        print(\"Hey Um sample losees: \", sample_losses)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print(\"loss:\", loss)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ">>>\n",
    "[[0.33333334 0.33333334 0.33333334]\n",
    " [0.33333316 0.3333332  0.33333364]\n",
    " [0.33333287 0.3333329  0.33333418]\n",
    " [0.3333326  0.33333263 0.33333477]\n",
    " [0.33333233 0.3333324  0.33333528]]\n",
    "loss: 1.0986104\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Probabilities of 3 samples\n",
    "softmax_output = np.array([[0.7, 0.2, 0.1], [0.5, 0.1, 0.4], [0.02, 0.9, 0.08]])\n",
    "\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# Calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_output, axis=1)\n",
    "# If targets are one-hot encoded - convert them\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "\n",
    "# True evaluates to 1; False to 0\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "print(\"acc:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "Hey Um sample losees:  [1.0986123 1.0986128 1.0986137 1.0986145 1.0986153 1.0986137 1.0986168\n",
      " 1.0986168 1.0986178 1.0986191 1.0986198 1.098619  1.0986207 1.098622\n",
      " 1.098622  1.0986223 1.0986184 1.0986241 1.0986224 1.098626  1.098626\n",
      " 1.0986195 1.0986171 1.0986255 1.0986353 1.0986279 1.098618  1.0986184\n",
      " 1.0986398 1.0986418 1.0986305 1.0986369 1.0986211 1.0986254 1.0986377\n",
      " 1.098647  1.0986525 1.098653  1.0986506 1.098654  1.0986497 1.0986483\n",
      " 1.0986478 1.0986323 1.0986618 1.098662  1.098664  1.0986414 1.0986663\n",
      " 1.0986534 1.098662  1.0986348 1.0986499 1.0986599 1.0986315 1.0986141\n",
      " 1.0986186 1.0986127 1.0986273 1.0986137 1.098617  1.098616  1.0986123\n",
      " 1.098631  1.0986419 1.0986344 1.0986123 1.0986576 1.0986377 1.0986589\n",
      " 1.0986655 1.0986648 1.0986633 1.0986542 1.0986674 1.0986677 1.0986685\n",
      " 1.0986708 1.0986696 1.0986639 1.0986731 1.0986444 1.0986496 1.0986748\n",
      " 1.0986496 1.0986749 1.0986547 1.0986434 1.0986588 1.0986764 1.0986344\n",
      " 1.0986977 1.0986687 1.0987039 1.098692  1.0987091 1.0986956 1.0987171\n",
      " 1.0987128 1.098722  1.0986123 1.0986127 1.0986134 1.0986152 1.0986154\n",
      " 1.0986152 1.0986184 1.0986192 1.0986148 1.0986161 1.0986137 1.0986179\n",
      " 1.0986241 1.098613  1.0986227 1.0986148 1.0986123 1.0986198 1.0986123\n",
      " 1.0986149 1.0986127 1.0986229 1.0986152 1.0986241 1.0986125 1.098617\n",
      " 1.0986165 1.0986314 1.0986289 1.0986291 1.0986218 1.098633  1.0986272\n",
      " 1.0986339 1.0986319 1.0986371 1.0986376 1.0986384 1.0986383 1.0986383\n",
      " 1.0986379 1.0986363 1.0986376 1.0986311 1.0986297 1.0986218 1.098642\n",
      " 1.0986164 1.0986434 1.0986321 1.0986229 1.0986484 1.098628  1.0986263\n",
      " 1.0986216 1.0986159 1.0986596 1.0986164 1.098617  1.0986413 1.0986454\n",
      " 1.098671  1.0986749 1.0986711 1.0986499 1.098677  1.098666  1.0986592\n",
      " 1.0986667 1.0986786 1.0986652 1.0986745 1.0986577 1.0986841 1.0986863\n",
      " 1.0986457 1.098681  1.0986754 1.0986645 1.0986422 1.0986489 1.0986756\n",
      " 1.0986454 1.0986873 1.0986316 1.0986601 1.0986327 1.0986334 1.0986155\n",
      " 1.0986735 1.0986123 1.0986614 1.0986596 1.0986346 1.0986767 1.0986708\n",
      " 1.0986806 1.0986805 1.0986531 1.0986693 1.0986123 1.0986109 1.09861\n",
      " 1.0986086 1.0986077 1.0986053 1.0986048 1.0986024 1.0986047 1.0986029\n",
      " 1.098609  1.0986058 1.0986084 1.0986017 1.0985988 1.0986042 1.098607\n",
      " 1.0985962 1.0985739 1.0985948 1.0986065 1.0985909 1.0985881 1.0985682\n",
      " 1.0986019 1.0985645 1.0985594 1.0985554 1.0985595 1.0985516 1.0985633\n",
      " 1.0985466 1.098544  1.0985512 1.0985405 1.0985655 1.098593  1.0985372\n",
      " 1.0985689 1.0985836 1.0985469 1.0985584 1.0985733 1.0985739 1.0985808\n",
      " 1.0985379 1.0985814 1.0985769 1.098592  1.0985705 1.0985731 1.0986037\n",
      " 1.0985708 1.0985787 1.0985433 1.0985383 1.0985698 1.0985862 1.0985857\n",
      " 1.0985255 1.0985543 1.0985299 1.0985267 1.0985194 1.0985483 1.098522\n",
      " 1.0985324 1.0985252 1.0985197 1.0985336 1.0985484 1.0985223 1.0985361\n",
      " 1.0985708 1.098536  1.0985528 1.0985816 1.0985836 1.0985138 1.0984505\n",
      " 1.0985631 1.0984706 1.0985769 1.0984368 1.0984722 1.098446  1.0985355\n",
      " 1.098427  1.0984246 1.0984327 1.098446  1.0984297 1.0985247 1.0984324\n",
      " 1.0984129 1.0984786 1.0984267 1.0984656 1.0984188 1.098476 ]\n",
      "loss: 1.0986104\n",
      "acc: 0.34\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        print(\"Hey Um sample losees: \", sample_losses)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print(\"loss:\", loss)\n",
    "\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# Calculate values along first axis\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"acc:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
