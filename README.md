# Neural Network from Scratch in Python

This project is a deep dive into the fundamentals of neural networks, aimed at building a neural network from the ground up using Python. Inspired by the concepts introduced in [this book](https://nnfs.io/), the goal was to understand and implement the core components of neural networks, including layers, activation functions, loss functions, and optimization algorithms, without relying on high-level libraries.

## Project Overview

The repository contains a series of chapters, each focused on a different aspect of neural network development. Starting with basic neuron coding, the project progresses through adding layers, implementing various activation functions, designing loss functions, and optimizing the network using different strategies. Advanced topics such as regularization, dropout, and working with real datasets are also covered.

Key features:
- **Custom Activation Functions:** Implementation of ReLU, Sigmoid, and Softmax from scratch.
- **Loss Function Exploration:** Development of loss functions like Cross-Entropy and Mean Squared Error without library support.
- **Optimization Techniques:** Introduction to optimizers like SGD, Adam, and RMSprop, coded natively.
- **Regularization and Dropout:** Integration of L1/L2 regularization and dropout techniques to combat overfitting.
- **Real Dataset Application:** Application of the neural network to classify images from the Fashion MNIST dataset.
- **Visualization of Weight Modifications:** New feature added to visualize the modifications of weights in each layer and their average per class, enhancing understanding of the model's learning process.

## Getting Started

Please refer to `environment.yml` for the project dependencies. Each chapter (`CH2_` to `CH22_`) is self-contained with both Python scripts and Jupyter notebooks, providing a comprehensive guide through each step of the neural network construction.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
